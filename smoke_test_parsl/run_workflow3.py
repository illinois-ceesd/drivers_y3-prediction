#!/usr/bin/env python3

import parsl
from parsl.config import Config
from parsl.providers import LSFProvider, SlurmProvider
from parsl.executors import HighThroughputExecutor, ThreadPoolExecutor
from parsl.launchers import JsrunLauncher, SrunLauncher
from parsl.addresses import address_by_hostname
from parsl.app.app import bash_app, python_app

executor_name = "my_executor"
host = address_by_hostname()
if 'lassen' in host.lower():
    executor = HighThroughputExecutor(
        label=executor_name,
        address=address_by_hostname(),
        worker_debug=True,
        provider=LSFProvider(launcher=JsrunLauncher(overrides=''),
            walltime='01:00:00',
            nodes_per_block=1,
            init_blocks=1,
            max_blocks=1,
            bsub_redirection=True,
            queue='pdebug',
            worker_init=(
                "module load spectrum-mpi\n"
                "source emirge/miniforge3/bin/activate mirgeDriver.Y3prediction\n"
                "export PYOPENCL_CTX=port:tesla\n"
                "export XDG_CACHE_HOME=/tmp/$USER/xdg-scratch\n"),
            project='uiuc')
        )
elif 'quartz' in host.lower():
    executor = HighThroughputExecutor(label=executor_name,
        address=address_by_hostname(),
        worker_debug=True,
        provider=SlurmProvider(launcher=SrunLauncher(overrides=''),
            walltime='01:00:00',
            nodes_per_block=1,
            init_blocks=1,
            max_blocks=1,
            scheduler_options='#SBATCH -q pdebug',
            worker_init=(
                "module load spectrum-mpi\n"
                "source emirge/miniforge3/bin/activate mirgeDriver.Y3prediction\n"
                "export XDG_CACHE_HOME=/tmp/$USER/xdg-scratch\n"),
            )
        )
else:
    #executor = ThreadPoolExecutor(label=executor_name, max_threads=5)
    from parsl.channels import LocalChannel
    from parsl.providers import LocalProvider
    executor = HighThroughputExecutor(
        label=executor_name,
        worker_debug=True,
        cores_per_worker=1,
        provider=LocalProvider(
            channel=LocalChannel(),
            init_blocks=1,
            max_blocks=1,
        ),
    )

# used for low priority tasks, not parallel
local_executor_name = "local_executor"
local_executor = ThreadPoolExecutor(label=local_executor_name, max_threads=5)
config = Config(executors=[executor, local_executor], strategy=None)
parsl.load(config)


@bash_app(executors=[executor_name])
def execute(execution_string="",
            stdout="stdout.txt", stderr="stderr.txt",
            inputs=[], outputs=[]):
    return(execution_string)


def build_execution_string(module="driver.py", yml="run_params.yaml",
                           c=None, t=None, r=None,
                           lazy=True, log=True):
    """ Generate and return the driver command line based on inputs

        Parameters
        ----------
        module: the python module to run (default is "driver.py")
        yml: the name of the yaml comtrol file (default is "run_params.yaml")
        c: the case name to pass to the -c argument of the module
        t: the file name to pass to the -t argument of the module
        r: the file name to pass to the -r argument of the module
        lazy: boolean, whether to set the lazy flag for the module
        log: boolean, whether to disable logpyle
    """

    # can't have mpirun here, need to let the executor set whatever that should be
    cmd = f"mpirun -n 2 python -u -O -m mpi4py {module} -i {yml}"
    if lazy:
        cmd += " --lazy"
    if not log:
        cmd += " --nolog"
    if t:
        cmd += f" -t {t}"
    if r:
        cmd += f" -r {r}"
    if c:
        cmd += f" -c {c}"
    print(f"\n\n{cmd}\n\n")
    return cmd


@python_app(executors=[local_executor_name])
def monitor_restart(file, start_time, outputs=[]):
    """ Monitor the restart directory for existence of a file

        Parameters
        ----------
        file: the file to look for
        outputs: list of parsl.File objects generated by this operation (given as an empty list and populated by this function)

    """
    import os
    #from parsl.data_provider.files import File
    import time
    import datetime
    target_file = f"{os.getcwd()}/restart_data/{file}"
    abort_file = f"{os.getcwd()}/stop_workflow"
    filenotfound = True
    delay = 5

    while filenotfound:
        print(f"Waiting for {target_file}")
        if os.path.isfile(target_file):
            # ensure that it has been modified since run inception
            target_time = os.path.getmtime(target_file)
            if target_time > start_time:
                filenotfound = False

        if os.path.isfile(abort_file):
            filenotfound = False

        time.sleep(delay)
    return 0


def main():

    #######
    # 1. run mirgecom and generate restart_data
    #######

    # we need to know a priori what the outfiles files generated will be,
    # so parsl can generate futures for data dependency
    run_restart_files = ["restart_data/prediction-000000000-0000.pkl",
                         "restart_data/prediction-000000005-0000.pkl",
                         "restart_data/prediction-000000010-0000.pkl",
                         "restart_data/prediction-000000015-0000.pkl",
                         "restart_data/prediction-000000020-0000.pkl"]

    parsl_restart_outfile = []
    from parsl.data_provider.files import File
    import os
    for restart_file in run_restart_files:
        parsl_restart_outfile.append(File(os.path.join(os.getcwd(), restart_file),))

    """
    init_restart_file = File(os.path.join(os.getcwd(), 'flame1d-000000-0000.pkl'))
    intro_str = 'echo "Running flame1d_init"\n'
    conda_str = load_conda()
    execution_str = 'python -u -m mpi4py flame_init.py\n'
    ex_str = intro_str+conda_str+execution_str
    stdout_str = 'flame1d_init.stdout'
    stderr_str = 'flame1d_init.stdout'
    flame_init = run_mirge(execution_string=ex_str, stdout=stdout_str, stderr=stderr_str, outputs=[init_restart_file])
    """

    import datetime
    start_time = datetime.datetime.timestamp(datetime.datetime.now())
    mirge_cmd = build_execution_string(
        yml="run_params.yaml", lazy=False, log=True)
    run_mirge = execute(execution_string=mirge_cmd,
                        stderr="run_stderr.txt",
                        stdout="run_stdout.txt",
                        outputs=parsl_restart_outfile)

    #run_mirge.result()  # wait until it is done
    print(run_mirge)
    print(run_mirge.outputs)

    #######
    # 2. monitor the mirgecom restart files to determine when they become available
    #    we do this seperately from the run future, as it won't tell us the files
    #    are complete until the app completes
    #######

    monitor_restart_data = []
    for future in run_mirge.outputs:
        print(future.filename)

        # run mirge and make the viz files
        monitor_restart_data.append(
            monitor_restart(
                file=os.path.basename(future.filename),
                start_time=start_time,
                #outputs=[File(f"{future.filename}.exists")]
                outputs=[File(future.filename)]
            )
        )

    print(monitor_restart_data)

    #######
    # 3. run mirgecom and generate viz_data from restart data
    #######
    make_viz_data = []
    for app_future in monitor_restart_data:
        # first figure out the names of the viz files to be created
        # so we can make data futures for them
        #print(future.filename)
        #print(app_future)
        #print(app_future.outputs)
        future = app_future.outputs[0]
        #print(future.filename)
        restart_name = os.path.basename(future.filename)
        # remove extension
        restart_name = os.path.splitext(restart_name)[0]
        # remove rank number
        restart_name = restart_name[:len(restart_name)-5]
        # get dump number and the casename
        dump_number = restart_name[len(restart_name)-9:]
        case_name = restart_name[:len(restart_name)-10]
        #print(restart_name)
        #print(case_name)
        #print(dump_number)
        # construct dump name
        viz_name_fluid = (f"viz_data/{case_name}-fluid-{dump_number}.pvtu")
        viz_name_wall = (f"viz_data/{case_name}-wall-{dump_number}.pvtu")
        #print(viz_name_fluid)
        #print(viz_name_wall)

        parsl_viz_outfile = []
        parsl_viz_outfile.append(File(os.path.join(os.getcwd(), viz_name_fluid),))
        parsl_viz_outfile.append(File(os.path.join(os.getcwd(), viz_name_wall),))

        # run mirge and make the viz files
        mirge_viz_cmd = build_execution_string(
            yml="make_viz_params.yaml",
            r=f"restart_data/{restart_name}",
            log=False,
            lazy=False)
        make_viz_data.append(
            execute(execution_string=mirge_viz_cmd,
                    stderr="viz_stderr.txt",
                    stdout="viz_stdout.txt",
                    inputs=[future],
                    outputs=parsl_viz_outfile)
        )
        """
        make_viz_data.append(mpirun(directory=f"viz {dump_number}",
                                    yml="make_viz_params.yaml",
                                    r=f"restart_data/{restart_name}",
                                    inputs=[future],
                                    outputs=parsl_viz_outfile,
                                    lazy=False))
        """

    #make_viz_data[0].result()  # wait until it is done
    #print(make_viz_data[0].outputs)
    print("waiting for viz to finish?")
    print(len(make_viz_data))
    print(make_viz_data)
    loop_ind = 0
    for i in make_viz_data:
        print(f"{loop_ind=}")
        print(i)
        print(i.task_status())
        i.result()
        print("after result")
        print(i.outputs)
        print("after outputs")
        loop_ind = loop_ind + 1

    #viz_output = [i.result() for i in make_viz_data]
    #print(viz_output)

    run_mirge.result()  # wait until mirge execution is done

if __name__ == "__main__":
    main()
